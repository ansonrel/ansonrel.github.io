{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Omnibenchmark documentation Omnibenchmark is a benchmark project that aims to provide community-driven , modular , extensible and always up-to-date benchmarks. It is based on the Renku project , an open and collaborative data analysis platform. The framework connects data , methods and metrics repositories (a.k.a. modules ), that can be flexibly extended by any community member. If you are interested in contributing a new method , dataset or metric , follow the documentation from the Getting started section, where you can learn more about how omnibenchmark works and how to extend it with a new module. If you are interested in exploring one of the existing benchmarks and the latest results, you can directly jump to the results of our evaluations in the Output section.","title":"Home"},{"location":"#welcome-to-omnibenchmark-documentation","text":"Omnibenchmark is a benchmark project that aims to provide community-driven , modular , extensible and always up-to-date benchmarks. It is based on the Renku project , an open and collaborative data analysis platform. The framework connects data , methods and metrics repositories (a.k.a. modules ), that can be flexibly extended by any community member. If you are interested in contributing a new method , dataset or metric , follow the documentation from the Getting started section, where you can learn more about how omnibenchmark works and how to extend it with a new module. If you are interested in exploring one of the existing benchmarks and the latest results, you can directly jump to the results of our evaluations in the Output section.","title":"Welcome to Omnibenchmark documentation"},{"location":"01_landing/","text":"Motivation Benchmarking is a critical step for the development of bioinformatic methods and provides important insights for their application. The current benchmarking scheme has many limitations: It is a snapshot of the available methods at a certain time point and often already outdated when published. Comparison of benchmarks is challenging: different procedures, different datasets, different evaluation criteria, etc. All of the above can lead to different conclusions among benchmarks made at different time points or at different groups. Concept Omnibenchmark is a modular and extensible framework based on a free open-source analytic platform, Renku , to offer a continuous and open community benchmarking system. The framework consists of data, method and metric repositories (or \u201cmodules\u201d) that share data components and are tracked via a knowledge graph from the Renku system. The results can be displayed in an interactive dashboard to be openly explored by anyone looking for recommendations of tools. New data, methods or metrics can be added by the community to extend the benchmark. Some key features of our benchmarking framework: Periodical updates of the benchmark to provide up-to-date results Easy extensibility through templates for data, methods or metrics Following FAIR principles by using software containers, an integration with Gitlab and full provenance tracking (inputs, workflows and generated files) Flexibility to work with different benchmarking structures, topics and programming languages. Prototype We are currently building a prototype for community-based benchmarking of single cell batch correction methods. The research in single-cell is a perfect use-case, where more than 1000 tools have been developed in only a few years (see for e.g. scrna-tools ) and where the benchmarking efforts are often not coordinated , not extendable and not reproducible .","title":"Motivation"},{"location":"01_landing/#motivation","text":"Benchmarking is a critical step for the development of bioinformatic methods and provides important insights for their application. The current benchmarking scheme has many limitations: It is a snapshot of the available methods at a certain time point and often already outdated when published. Comparison of benchmarks is challenging: different procedures, different datasets, different evaluation criteria, etc. All of the above can lead to different conclusions among benchmarks made at different time points or at different groups.","title":"Motivation"},{"location":"01_landing/#concept","text":"Omnibenchmark is a modular and extensible framework based on a free open-source analytic platform, Renku , to offer a continuous and open community benchmarking system. The framework consists of data, method and metric repositories (or \u201cmodules\u201d) that share data components and are tracked via a knowledge graph from the Renku system. The results can be displayed in an interactive dashboard to be openly explored by anyone looking for recommendations of tools. New data, methods or metrics can be added by the community to extend the benchmark. Some key features of our benchmarking framework: Periodical updates of the benchmark to provide up-to-date results Easy extensibility through templates for data, methods or metrics Following FAIR principles by using software containers, an integration with Gitlab and full provenance tracking (inputs, workflows and generated files) Flexibility to work with different benchmarking structures, topics and programming languages.","title":"Concept"},{"location":"01_landing/#prototype","text":"We are currently building a prototype for community-based benchmarking of single cell batch correction methods. The research in single-cell is a perfect use-case, where more than 1000 tools have been developed in only a few years (see for e.g. scrna-tools ) and where the benchmarking efforts are often not coordinated , not extendable and not reproducible .","title":"Prototype"},{"location":"about/","text":"Indetonsusque loca est epulas saeva Calidi seu Lorem markdownum, nunc aret fragorem sorori, dea antemnas sinat, Philippi, auctor, adfixa! Aere Venulus, me dubitare fronte peregrina feruntur et iussit resolvite? Nocte cervix truncos recessit, apri his fatemur timori Tantalis, et haec metum clipeum. Non nec comitatur palmae redolentia tamen aut Queri voce oblivia subito, ex timentes utque: ferox diligitur candidus veros . Vixque nostri nec est inpulsos cupiuntque in color, terris mihi oves gloria comae aut Salmaci ? Inde causa tantos toro: non imo ab atque; abstrahit intexere. Saxum de totaque certe fecit nive triplici, si nam cum meo feroci diu culpa canibusve magna. Sine fatemur versa perdite est isto trabes Facundia dixit huic nefasque decimo suae dixit non. Rectum dumque acies qua, dixerat , fitque, tali tuum bellica! Belua poenaededidit lecti ferunt fatale mutentur sacerdos alta, ab nec dis. Nam et Dianae vitare Numam, esse cum! Qui et Lycaon omnes: dixi oblita? Olorinis summum Colunt breviter Saturnia Threicius motasse aestus tibi obliquo maesto caeruleaeque alti, qui cum vias tenet pelagi noctem? Uni axe ossa, laesi, sed ducere tantum caducas atricolor vicinia pervia! Veteris abit iussae ictus cohaesit Alter diro Scyllae suoque superat nullos Cumarum Plangoris prohibebant deferre capillis Ponit ante nemus Ait parte quem Hymenaeus Referrem per mihi spiritus, praecipites Somni quoque, oppositoque. Cum occuluit Iovem ire: pulcherrime neque frondes vultumque anili, sic habet sustulerat quam ad nymphae Haemonias ostendit! Lapis quaerens potitur carpitur iam, terra ima Autolycus Aegaeo, Tegeaea et ! Etiam nitidum transire Bromiumque vestrum; thalami enim lateri profundum aconita nec!","title":"About"},{"location":"about/#indetonsusque-loca-est-epulas-saeva","text":"","title":"Indetonsusque loca est epulas saeva"},{"location":"about/#calidi-seu","text":"Lorem markdownum, nunc aret fragorem sorori, dea antemnas sinat, Philippi, auctor, adfixa! Aere Venulus, me dubitare fronte peregrina feruntur et iussit resolvite? Nocte cervix truncos recessit, apri his fatemur timori Tantalis, et haec metum clipeum.","title":"Calidi seu"},{"location":"about/#non-nec-comitatur-palmae-redolentia-tamen-aut","text":"Queri voce oblivia subito, ex timentes utque: ferox diligitur candidus veros . Vixque nostri nec est inpulsos cupiuntque in color, terris mihi oves gloria comae aut Salmaci ? Inde causa tantos toro: non imo ab atque; abstrahit intexere. Saxum de totaque certe fecit nive triplici, si nam cum meo feroci diu culpa canibusve magna.","title":"Non nec comitatur palmae redolentia tamen aut"},{"location":"about/#sine-fatemur-versa-perdite-est-isto-trabes","text":"Facundia dixit huic nefasque decimo suae dixit non. Rectum dumque acies qua, dixerat , fitque, tali tuum bellica! Belua poenaededidit lecti ferunt fatale mutentur sacerdos alta, ab nec dis. Nam et Dianae vitare Numam, esse cum! Qui et Lycaon omnes: dixi oblita?","title":"Sine fatemur versa perdite est isto trabes"},{"location":"about/#olorinis-summum","text":"Colunt breviter Saturnia Threicius motasse aestus tibi obliquo maesto caeruleaeque alti, qui cum vias tenet pelagi noctem? Uni axe ossa, laesi, sed ducere tantum caducas atricolor vicinia pervia! Veteris abit iussae ictus cohaesit Alter diro Scyllae suoque superat nullos Cumarum Plangoris prohibebant deferre capillis Ponit ante nemus Ait parte quem Hymenaeus Referrem per mihi spiritus, praecipites Somni quoque, oppositoque. Cum occuluit Iovem ire: pulcherrime neque frondes vultumque anili, sic habet sustulerat quam ad nymphae Haemonias ostendit! Lapis quaerens potitur carpitur iam, terra ima Autolycus Aegaeo, Tegeaea et ! Etiam nitidum transire Bromiumque vestrum; thalami enim lateri profundum aconita nec!","title":"Olorinis summum"},{"location":"old_index/","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Welcome to MkDocs"},{"location":"old_index/#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"old_index/#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"old_index/#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"advanced/01_utils/","text":"Utils some text.","title":"Utils"},{"location":"advanced/01_utils/#utils","text":"some text.","title":"Utils"},{"location":"advanced/02_orchestrator/","text":"CI/CD Orchestrator. Some text.","title":"CI/CD Orchestrator."},{"location":"advanced/02_orchestrator/#cicd-orchestrator","text":"Some text.","title":"CI/CD Orchestrator."},{"location":"advanced/03_components/","text":"Components dashboard. Some text.","title":"Components dashboard."},{"location":"advanced/03_components/#components-dashboard","text":"Some text.","title":"Components dashboard."},{"location":"how_to/01_build_object/","text":"(section-build-yaml)= Build an OmniObject from yaml All relevant information on how to run a specific module are stored as {ref} OmniObject <section-omniobject> . The most convenient way to generate an instance of an OmniObject is to build it from a config.yaml file using the get_omni_object_from_yaml() function: ## modules from omnibenchmark.utils.build_omni_object import get_omni_object_from_yaml ## Load object omni_obj = get_omni_object_from_yaml('src/config.yaml')","title":"01 build object"},{"location":"how_to/01_build_object/#build-an-omniobject-from-yaml","text":"All relevant information on how to run a specific module are stored as {ref} OmniObject <section-omniobject> . The most convenient way to generate an instance of an OmniObject is to build it from a config.yaml file using the get_omni_object_from_yaml() function: ## modules from omnibenchmark.utils.build_omni_object import get_omni_object_from_yaml ## Load object omni_obj = get_omni_object_from_yaml('src/config.yaml')","title":"Build an OmniObject from yaml"},{"location":"how_to/02_create_dataset/","text":"(section-datasets)= Datasets Generate Datasets Add files Update datasets","title":"02 create dataset"},{"location":"how_to/02_create_dataset/#datasets","text":"","title":"Datasets"},{"location":"how_to/02_create_dataset/#generate-datasets","text":"","title":"Generate Datasets"},{"location":"how_to/02_create_dataset/#add-files","text":"","title":"Add files"},{"location":"how_to/02_create_dataset/#update-datasets","text":"","title":"Update datasets"},{"location":"how_to/03_generate_workflow/","text":"(section-workflow)= Generate and update workflows","title":"03 generate workflow"},{"location":"how_to/03_generate_workflow/#generate-and-update-workflows","text":"","title":"Generate and update workflows"},{"location":"how_to/04_update_object/","text":"(section-update)= Update OmniObject","title":"04 update object"},{"location":"how_to/04_update_object/#update-omniobject","text":"","title":"Update OmniObject"},{"location":"how_to/05_filter/","text":"(section-filter)= Filter OmniObject Filter input datasets Filter parameter Parameter limits Parameter values Parameter combinations Input dataset specific parameter combinations","title":"05 filter"},{"location":"how_to/05_filter/#filter-omniobject","text":"","title":"Filter OmniObject"},{"location":"how_to/05_filter/#filter-input-datasets","text":"","title":"Filter input datasets"},{"location":"how_to/05_filter/#filter-parameter","text":"","title":"Filter parameter"},{"location":"how_to/05_filter/#parameter-limits","text":"","title":"Parameter limits"},{"location":"how_to/05_filter/#parameter-values","text":"","title":"Parameter values"},{"location":"how_to/05_filter/#parameter-combinations","text":"","title":"Parameter combinations"},{"location":"how_to/05_filter/#input-dataset-specific-parameter-combinations","text":"","title":"Input dataset specific parameter combinations"},{"location":"output/01_prototype_batch/","text":"Batch correction Results of our protype, evaluating: two datasets two batch-correction methods two different metrics. (http://imlspenticton.uzh.ch:3840/bettr_test/)[http://imlspenticton.uzh.ch:3840/bettr_test/]","title":"Batch correction"},{"location":"output/01_prototype_batch/#batch-correction","text":"Results of our protype, evaluating: two datasets two batch-correction methods two different metrics. (http://imlspenticton.uzh.ch:3840/bettr_test/)[http://imlspenticton.uzh.ch:3840/bettr_test/]","title":"Batch correction"},{"location":"start/01_project_setup/","text":"Project setup (section-start-project)= Start a renku project Each module of omnibenchmark is an independent renku project. It can be setup as a new GitLab project with the following features: A Dockerfile to specify the computational environment to run your project in. A gitlab-ci.yaml file to continuously build and update the projects Docker container and module script. Git LFS for large file storage. Further project organisation files like a .gitignore , .gitattributes , .renkulfsignore , .dockerignore . Having independent modules also means that you can test your code and work on your project without being included into an existing benchmark. Only when a project is included into an omnibenchmark orchestrator it becomes part of the benchmark itself (explained latter Omnibenchmark modules) . To start a new renku project login at the renku with your GitHub account, OrchID or SWITCH-eduID or register a renku account. :::info Start a new renku project : renkulab.io -> projects -> + New project ::: Select a name, namespace, description and suitable template and create a new project. There are no specific requirements for omnibenchmark at this stage. Read more about renku projects here . Specify your modules environment A module environment can be specified by modifying the Dockerfile . To run omnibenchmark you need the python modules renku-python and omnibenchmark installed. There are no further requirements, but it can be useful to follow the template structure in the automatically generated Dockerfile . Depending on what template you chose a renku base image will be selected at the top. Make sure this base image is reasonable for your module (e.g. choose one with R installed if your module calls R code). Extra linux (ubuntu) software requirements can be specified within the Dockerfile , while python modules are typically defined in the requirements.txt file or using conda\u2019s environment management system in the environment.yaml file and installation of R packages is specified in the install.R file. Detailed instruction can be found here . The automatically generated Dockerfile already contains commands to install renku-python at the bottom, but you need to specify omnibenchmark with the version you want to use as software requirement: :::info Add this line to requirements.txt : omnibenchmark==VERSION ::: If you want to call R code and you chose a R template when creating the renku project the install.R file is automatically generated upon project creation. Otherwise make sure you switch to a base image with R installed and add the install.R file manually, as well as the following lines to your Dockerfile : # install the R dependencies COPY install.R /tmp/ RUN R -f /tmp/install.R Run your code After each commit renku builds automatically a Docker container with your specified requirements. You can start an interactive session using the latest container at renkulab.io or work with renku on your own machine .","title":"Project setup"},{"location":"start/01_project_setup/#project-setup","text":"(section-start-project)=","title":"Project setup"},{"location":"start/01_project_setup/#start-a-renku-project","text":"Each module of omnibenchmark is an independent renku project. It can be setup as a new GitLab project with the following features: A Dockerfile to specify the computational environment to run your project in. A gitlab-ci.yaml file to continuously build and update the projects Docker container and module script. Git LFS for large file storage. Further project organisation files like a .gitignore , .gitattributes , .renkulfsignore , .dockerignore . Having independent modules also means that you can test your code and work on your project without being included into an existing benchmark. Only when a project is included into an omnibenchmark orchestrator it becomes part of the benchmark itself (explained latter Omnibenchmark modules) . To start a new renku project login at the renku with your GitHub account, OrchID or SWITCH-eduID or register a renku account. :::info Start a new renku project : renkulab.io -> projects -> + New project ::: Select a name, namespace, description and suitable template and create a new project. There are no specific requirements for omnibenchmark at this stage. Read more about renku projects here .","title":"Start a renku project"},{"location":"start/01_project_setup/#specify-your-modules-environment","text":"A module environment can be specified by modifying the Dockerfile . To run omnibenchmark you need the python modules renku-python and omnibenchmark installed. There are no further requirements, but it can be useful to follow the template structure in the automatically generated Dockerfile . Depending on what template you chose a renku base image will be selected at the top. Make sure this base image is reasonable for your module (e.g. choose one with R installed if your module calls R code). Extra linux (ubuntu) software requirements can be specified within the Dockerfile , while python modules are typically defined in the requirements.txt file or using conda\u2019s environment management system in the environment.yaml file and installation of R packages is specified in the install.R file. Detailed instruction can be found here . The automatically generated Dockerfile already contains commands to install renku-python at the bottom, but you need to specify omnibenchmark with the version you want to use as software requirement: :::info Add this line to requirements.txt : omnibenchmark==VERSION ::: If you want to call R code and you chose a R template when creating the renku project the install.R file is automatically generated upon project creation. Otherwise make sure you switch to a base image with R installed and add the install.R file manually, as well as the following lines to your Dockerfile : # install the R dependencies COPY install.R /tmp/ RUN R -f /tmp/install.R","title":"Specify your modules environment"},{"location":"start/01_project_setup/#run-your-code","text":"After each commit renku builds automatically a Docker container with your specified requirements. You can start an interactive session using the latest container at renkulab.io or work with renku on your own machine .","title":"Run your code"},{"location":"start/modules/01_data_module/","text":"Data modules Data modules are modules that define input datasets and bundle them into a renku dataset , that can be imported by other projects. Benchmark specific requirements like file formats, types and prefixes can be checked at the omnibenchmark webside . Most modules contain 3 main files: 1. The config.yaml file All information about the module are specified in the {ref} config.yaml file <section-config> : --- data: name: \"dataset-name\" title: \"dataset title\" description: \"A new dataset module for omnibenchmark\" keywords: [\"MODULE_KEY\"] script: \"path/to/module_script\" outputs: files: data_file1: end: \"FILE1_END\" data_file2: end: \"FILE2_END\" data_file3: end: \"FILE3_END\" benchmark_name: \"OMNIBENCHMARK_TYPE\" Entries in capital letters depend on the specifications at the omnibenchmark webside . 2. The run_workflow.py file This file is to generate, run and update the modules dataset and workflow. A most basic script to do so looks like this: # Load modules from omnibenchmark.utils.build_omni_object import get_omni_object_from_yaml from omnibenchmark.renku_commands.general import renku_save # Build an OmniObject from the config.yaml file omni_obj = get_omni_object_from_yaml('src/config.yaml') # Create the output dataset omni_obj.create_dataset() renku_save() ## Run and update the workflow omni_obj.run_renku() renku_save() ## Add files to output dataset omni_obj.update_result_dataset() renku_save() 3. The module script This is the script to load the dataset and to convert its files into the expected format. Omnibenchmark accepts any kind of script and its maintenance and content is up to the module author. Omnibenchmark calls this script from the command line. If you use another language than R, Python, Julia or bash, specify the interpreter to use in the corresponding field of the {ref} config.yaml file <section-config> file. :::note All input and output files and if applicable parameter need to be parsed from the command line in the format: --ARGUMENT_NAME ARGUMENT_VALUE ::: In Python argparse can be used to parse command arguments like this: # Load module import argparse # Get command line arguments and store them in args parser=argparse.ArgumentParser() parser.add_argument('--argument_name', help='Description of the argument') args=parser.parse_args() # Call the argument arg1 = args.argument_name In R we recommend to use the optparse package: # Load package library(optparse) # Get list with command line arguments by name option_list = list( make_option(c(\"--argument_name\"), type=\"character\", default=NULL, help=\"Description of the argument\", metavar=\"character\") ); opt_parser = OptionParser(option_list=option_list); opt = parse_args(opt_parser); # An useful error if the argument is missing if (is.null(opt$argument_name)){ print_help(opt_parser) stop(\"Argument_name needs to be specified, but is missing.n\", call.=FALSE) } # Call the argument arg1 <- opt$argument_name","title":"Data modules"},{"location":"start/modules/01_data_module/#data-modules","text":"Data modules are modules that define input datasets and bundle them into a renku dataset , that can be imported by other projects. Benchmark specific requirements like file formats, types and prefixes can be checked at the omnibenchmark webside . Most modules contain 3 main files:","title":"Data modules"},{"location":"start/modules/01_data_module/#1-the-configyaml-file","text":"All information about the module are specified in the {ref} config.yaml file <section-config> : --- data: name: \"dataset-name\" title: \"dataset title\" description: \"A new dataset module for omnibenchmark\" keywords: [\"MODULE_KEY\"] script: \"path/to/module_script\" outputs: files: data_file1: end: \"FILE1_END\" data_file2: end: \"FILE2_END\" data_file3: end: \"FILE3_END\" benchmark_name: \"OMNIBENCHMARK_TYPE\" Entries in capital letters depend on the specifications at the omnibenchmark webside .","title":"1. The config.yaml file"},{"location":"start/modules/01_data_module/#2-the-run_workflowpy-file","text":"This file is to generate, run and update the modules dataset and workflow. A most basic script to do so looks like this: # Load modules from omnibenchmark.utils.build_omni_object import get_omni_object_from_yaml from omnibenchmark.renku_commands.general import renku_save # Build an OmniObject from the config.yaml file omni_obj = get_omni_object_from_yaml('src/config.yaml') # Create the output dataset omni_obj.create_dataset() renku_save() ## Run and update the workflow omni_obj.run_renku() renku_save() ## Add files to output dataset omni_obj.update_result_dataset() renku_save()","title":"2. The run_workflow.py file"},{"location":"start/modules/01_data_module/#3-the-module-script","text":"This is the script to load the dataset and to convert its files into the expected format. Omnibenchmark accepts any kind of script and its maintenance and content is up to the module author. Omnibenchmark calls this script from the command line. If you use another language than R, Python, Julia or bash, specify the interpreter to use in the corresponding field of the {ref} config.yaml file <section-config> file. :::note All input and output files and if applicable parameter need to be parsed from the command line in the format: --ARGUMENT_NAME ARGUMENT_VALUE ::: In Python argparse can be used to parse command arguments like this: # Load module import argparse # Get command line arguments and store them in args parser=argparse.ArgumentParser() parser.add_argument('--argument_name', help='Description of the argument') args=parser.parse_args() # Call the argument arg1 = args.argument_name In R we recommend to use the optparse package: # Load package library(optparse) # Get list with command line arguments by name option_list = list( make_option(c(\"--argument_name\"), type=\"character\", default=NULL, help=\"Description of the argument\", metavar=\"character\") ); opt_parser = OptionParser(option_list=option_list); opt = parse_args(opt_parser); # An useful error if the argument is missing if (is.null(opt$argument_name)){ print_help(opt_parser) stop(\"Argument_name needs to be specified, but is missing.n\", call.=FALSE) } # Call the argument arg1 <- opt$argument_name","title":"3. The module script"},{"location":"start/modules/02_method_module/","text":"Method modules A method module imports all datasets of a benchmark or all preprocessed inputs and runs one benchmarking method on them. Method outputs are added to a renku dataset , that can be imported by metric projects to evaluate them. Benchmark specific requirements like file formats, types and prefixes can be checked at the omnibenchmark webside . To exlore a methods parameter space a parameter dataset is imported. Valid parameter can be specified in the config.yaml file <section-config> or specific {ref} filter <section-filter> files. There are {ref} filter <section-filter> at different level, e.g. parameter limits, specific values and parameter and input file combinations. Usually a method module contains only one workflow, that is automatically run with all valid parameter and input file combinations. Most modules contain 3 main files: 1. The config.yaml file All information about the module are specified in the {ref} config.yaml file <section-config> : --- data: name: \"method name\" title: \"method title\" description: \"Short description of the method\" keywords: [\"MODULE_KEY\"] script: \"path/to/module_script\" benchmark_name: \"OMNIBENCHMARK_TYPE\" inputs: keywords: [\"INPUT_KEY1\", \"INPUT_KEY2\"] files: [\"input_file_name1\", \"input_file_name2\"] prefix: input_file_name1: \"_INPUT1_\" input_file_name2: \"_INPUT2_\" outputs: files: method_result1: end: \"FILE1_END\" method_result2: end: \"FILE2_END\" parameter: names: [\"parameter1\", \"parameter2\"] keywords: [\"PARAMETER_KEY\"] Entries in capital letters depend on the specifications at the omnibenchmark webside . 2. The run_workflow.py file This file is to generate, run and update the modules dataset and workflow. A most basic script to do so looks like this: # Load modules from omnibenchmark.utils.build_omni_object import get_omni_object_from_yaml from omnibenchmark.renku_commands.general import renku_save # Build an OmniObject from the config.yaml file omni_obj = get_omni_object_from_yaml('src/config.yaml') # Import and update inputs/parameter and update object accordingly omni_obj.update_object() renku_save() # Create the output dataset omni_obj.create_dataset() renku_save() ## Run and update the workflow on all inputs and parameter combinations omni_obj.run_renku() renku_save() ## Add files to output dataset omni_obj.update_result_dataset() renku_save() 3. The module script This is the script to load the dataset and to convert its files into the expected format. Omnibenchmark accepts any kind of script and its maintenance and content is up to the module author. Omnibenchmark calls this script from the command line. If you use another language than R, Python, Julia or bash, specify the interpreter to use in the corresponding field of the {ref} config.yaml file <section-config> file. :::note All input and output files and if applicable parameter need to be parsed from the command line in the format: --ARGUMENT_NAME ARGUMENT_VALUE ::: In Python argparse can be used to parse command arguments like this: # Load module import argparse # Get command line arguments and store them in args parser=argparse.ArgumentParser() parser.add_argument('--argument_name', help='Description of the argument') args=parser.parse_args() # Call the argument arg1 = args.argument_name In R we recommend to use the optparse package: # Load package library(optparse) # Get list with command line arguments by name option_list = list( make_option(c(\"--argument_name\"), type=\"character\", default=NULL, help=\"Description of the argument\", metavar=\"character\") ); opt_parser = OptionParser(option_list=option_list); opt = parse_args(opt_parser); # An useful error if the argument is missing if (is.null(opt$argument_name)){ print_help(opt_parser) stop(\"Argument_name needs to be specified, but is missing.n\", call.=FALSE) } # Call the argument arg1 <- opt$argument_name","title":"Method modules"},{"location":"start/modules/02_method_module/#method-modules","text":"A method module imports all datasets of a benchmark or all preprocessed inputs and runs one benchmarking method on them. Method outputs are added to a renku dataset , that can be imported by metric projects to evaluate them. Benchmark specific requirements like file formats, types and prefixes can be checked at the omnibenchmark webside . To exlore a methods parameter space a parameter dataset is imported. Valid parameter can be specified in the config.yaml file <section-config> or specific {ref} filter <section-filter> files. There are {ref} filter <section-filter> at different level, e.g. parameter limits, specific values and parameter and input file combinations. Usually a method module contains only one workflow, that is automatically run with all valid parameter and input file combinations. Most modules contain 3 main files:","title":"Method modules"},{"location":"start/modules/02_method_module/#1-the-configyaml-file","text":"All information about the module are specified in the {ref} config.yaml file <section-config> : --- data: name: \"method name\" title: \"method title\" description: \"Short description of the method\" keywords: [\"MODULE_KEY\"] script: \"path/to/module_script\" benchmark_name: \"OMNIBENCHMARK_TYPE\" inputs: keywords: [\"INPUT_KEY1\", \"INPUT_KEY2\"] files: [\"input_file_name1\", \"input_file_name2\"] prefix: input_file_name1: \"_INPUT1_\" input_file_name2: \"_INPUT2_\" outputs: files: method_result1: end: \"FILE1_END\" method_result2: end: \"FILE2_END\" parameter: names: [\"parameter1\", \"parameter2\"] keywords: [\"PARAMETER_KEY\"] Entries in capital letters depend on the specifications at the omnibenchmark webside .","title":"1. The config.yaml file"},{"location":"start/modules/02_method_module/#2-the-run_workflowpy-file","text":"This file is to generate, run and update the modules dataset and workflow. A most basic script to do so looks like this: # Load modules from omnibenchmark.utils.build_omni_object import get_omni_object_from_yaml from omnibenchmark.renku_commands.general import renku_save # Build an OmniObject from the config.yaml file omni_obj = get_omni_object_from_yaml('src/config.yaml') # Import and update inputs/parameter and update object accordingly omni_obj.update_object() renku_save() # Create the output dataset omni_obj.create_dataset() renku_save() ## Run and update the workflow on all inputs and parameter combinations omni_obj.run_renku() renku_save() ## Add files to output dataset omni_obj.update_result_dataset() renku_save()","title":"2. The run_workflow.py file"},{"location":"start/modules/02_method_module/#3-the-module-script","text":"This is the script to load the dataset and to convert its files into the expected format. Omnibenchmark accepts any kind of script and its maintenance and content is up to the module author. Omnibenchmark calls this script from the command line. If you use another language than R, Python, Julia or bash, specify the interpreter to use in the corresponding field of the {ref} config.yaml file <section-config> file. :::note All input and output files and if applicable parameter need to be parsed from the command line in the format: --ARGUMENT_NAME ARGUMENT_VALUE ::: In Python argparse can be used to parse command arguments like this: # Load module import argparse # Get command line arguments and store them in args parser=argparse.ArgumentParser() parser.add_argument('--argument_name', help='Description of the argument') args=parser.parse_args() # Call the argument arg1 = args.argument_name In R we recommend to use the optparse package: # Load package library(optparse) # Get list with command line arguments by name option_list = list( make_option(c(\"--argument_name\"), type=\"character\", default=NULL, help=\"Description of the argument\", metavar=\"character\") ); opt_parser = OptionParser(option_list=option_list); opt = parse_args(opt_parser); # An useful error if the argument is missing if (is.null(opt$argument_name)){ print_help(opt_parser) stop(\"Argument_name needs to be specified, but is missing.n\", call.=FALSE) } # Call the argument arg1 <- opt$argument_name","title":"3. The module script"},{"location":"start/modules/03_metric_module/","text":"Metric modules A metric module imports method result datasets and runs one evaluation on them. Evaluation results are added to a renku dataset , that can be summarized and explored in an {ref} omnibenchmark bettr dashboard <section-output> . Benchmark specific requirements like file formats, types and prefixes can be checked at the omnibenchmark webside . Usually a metric module contains two workflows, one to evaluate the results and one to generate the metric info file . Most metric modules contain 5 main files: 1. The config.yaml file All information about the module are specified in the {ref} config.yaml file <section-config> : --- data: name: \"metric name\" title: \"metric title\" description: \"Short description of the metric\" keywords: [\"MODULE_KEY\"] script: \"path/to/module_script\" benchmark_name: \"OMNIBENCHMARK_TYPE\" inputs: keywords: [\"INPUT_KEY1\", \"INPUT_KEY2\"] files: [\"input_file_name1\", \"input_file_name2\"] prefix: input_file_name1: \"_INPUT1_\" input_file_name2: \"_INPUT2_\" outputs: files: metric_result: end: \"FILE1_END\" Entries in capital letters depend on the specifications at the omnibenchmark webside . 2. The info_config.yaml file All information about the module are specified in the {ref} config.yaml file <section-config> : --- data: name: \"metric name\" script: \"src/generate_metric_info.py\" benchmark_name: \"OMNIBENCHMARK_TYPE\" outputs: files: metric_info: end: \"json\" file_mapping: mapping1: output_files: metric_info: \"path/to/metric_name_info.json\" 3. The run_workflow.py file This file is to generate, run and update the modules dataset and workflow. A most basic script to do so looks like this: # Load modules from omnibenchmark.utils.build_omni_object import get_omni_object_from_yaml from omnibenchmark.renku_commands.general import renku_save # Build an OmniObject from the config.yaml file omni_obj = get_omni_object_from_yaml('src/config.yaml') # Import and update inputs/parameter and update object accordingly omni_obj.update_object() renku_save() # Create the output dataset omni_obj.create_dataset() renku_save() ## Run and update the workflow on all inputs and parameter combinations omni_obj.run_renku() renku_save() ## Add files to output dataset omni_obj.update_result_dataset() renku_save() ###################### Generate info file ###################### # Build an OmniObject from the info_config.yaml file omni_info = get_omni_object_from_yaml('src/info_config.yaml') omni_info.wflow_name = \"metric_info\" ## Run and update workflow omni_info.run_renku() renku_save() ## Update output dataset omni_info.update_result_dataset() renku_save() 4. The module script This is the script to load the dataset and to convert its files into the expected format. Omnibenchmark accepts any kind of script and its maintenance and content is up to the module author. Omnibenchmark calls this script from the command line. If you use another language than R, Python, Julia or bash, specify the interpreter to use in the corresponding field of the {ref} config.yaml file <section-config> file. :::note All input and output files and if applicable parameter need to be parsed from the command line in the format: --ARGUMENT_NAME ARGUMENT_VALUE ::: In Python argparse can be used to parse command arguments like this: # Load module import argparse # Get command line arguments and store them in args parser=argparse.ArgumentParser() parser.add_argument('--argument_name', help='Description of the argument') args=parser.parse_args() # Call the argument arg1 = args.argument_name In R we recommend to use the optparse package: # Load package library(optparse) # Get list with command line arguments by name option_list = list( make_option(c(\"--argument_name\"), type=\"character\", default=NULL, help=\"Description of the argument\", metavar=\"character\") ); opt_parser = OptionParser(option_list=option_list); opt = parse_args(opt_parser); # An useful error if the argument is missing if (is.null(opt$argument_name)){ print_help(opt_parser) stop(\"Argument_name needs to be specified, but is missing.n\", call.=FALSE) } # Call the argument arg1 <- opt$argument_name 5. The generate_metric_info.py file This file could also be written in R or any other language. It should return a json file with the below fields with the metrics information. import argparse import json parser=argparse.ArgumentParser() parser.add_argument('--metric_info', help='Path to the metric info json file') args=parser.parse_args() metric_info = { 'flip': False, 'max': 1, 'min': 0, 'group': \"METRIC_GROUP\", 'name': \"metric_name\", 'input': \"metric_input_type\" } with open(args.metric_info, \"w\") as fp: json.dump(metric_info , fp, indent=3)","title":"Metric modules"},{"location":"start/modules/03_metric_module/#metric-modules","text":"A metric module imports method result datasets and runs one evaluation on them. Evaluation results are added to a renku dataset , that can be summarized and explored in an {ref} omnibenchmark bettr dashboard <section-output> . Benchmark specific requirements like file formats, types and prefixes can be checked at the omnibenchmark webside . Usually a metric module contains two workflows, one to evaluate the results and one to generate the metric info file . Most metric modules contain 5 main files:","title":"Metric modules"},{"location":"start/modules/03_metric_module/#1-the-configyaml-file","text":"All information about the module are specified in the {ref} config.yaml file <section-config> : --- data: name: \"metric name\" title: \"metric title\" description: \"Short description of the metric\" keywords: [\"MODULE_KEY\"] script: \"path/to/module_script\" benchmark_name: \"OMNIBENCHMARK_TYPE\" inputs: keywords: [\"INPUT_KEY1\", \"INPUT_KEY2\"] files: [\"input_file_name1\", \"input_file_name2\"] prefix: input_file_name1: \"_INPUT1_\" input_file_name2: \"_INPUT2_\" outputs: files: metric_result: end: \"FILE1_END\" Entries in capital letters depend on the specifications at the omnibenchmark webside .","title":"1. The config.yaml file"},{"location":"start/modules/03_metric_module/#2-the-info_configyaml-file","text":"All information about the module are specified in the {ref} config.yaml file <section-config> : --- data: name: \"metric name\" script: \"src/generate_metric_info.py\" benchmark_name: \"OMNIBENCHMARK_TYPE\" outputs: files: metric_info: end: \"json\" file_mapping: mapping1: output_files: metric_info: \"path/to/metric_name_info.json\"","title":"2. The info_config.yaml file"},{"location":"start/modules/03_metric_module/#3-the-run_workflowpy-file","text":"This file is to generate, run and update the modules dataset and workflow. A most basic script to do so looks like this: # Load modules from omnibenchmark.utils.build_omni_object import get_omni_object_from_yaml from omnibenchmark.renku_commands.general import renku_save # Build an OmniObject from the config.yaml file omni_obj = get_omni_object_from_yaml('src/config.yaml') # Import and update inputs/parameter and update object accordingly omni_obj.update_object() renku_save() # Create the output dataset omni_obj.create_dataset() renku_save() ## Run and update the workflow on all inputs and parameter combinations omni_obj.run_renku() renku_save() ## Add files to output dataset omni_obj.update_result_dataset() renku_save() ###################### Generate info file ###################### # Build an OmniObject from the info_config.yaml file omni_info = get_omni_object_from_yaml('src/info_config.yaml') omni_info.wflow_name = \"metric_info\" ## Run and update workflow omni_info.run_renku() renku_save() ## Update output dataset omni_info.update_result_dataset() renku_save()","title":"3. The run_workflow.py file"},{"location":"start/modules/03_metric_module/#4-the-module-script","text":"This is the script to load the dataset and to convert its files into the expected format. Omnibenchmark accepts any kind of script and its maintenance and content is up to the module author. Omnibenchmark calls this script from the command line. If you use another language than R, Python, Julia or bash, specify the interpreter to use in the corresponding field of the {ref} config.yaml file <section-config> file. :::note All input and output files and if applicable parameter need to be parsed from the command line in the format: --ARGUMENT_NAME ARGUMENT_VALUE ::: In Python argparse can be used to parse command arguments like this: # Load module import argparse # Get command line arguments and store them in args parser=argparse.ArgumentParser() parser.add_argument('--argument_name', help='Description of the argument') args=parser.parse_args() # Call the argument arg1 = args.argument_name In R we recommend to use the optparse package: # Load package library(optparse) # Get list with command line arguments by name option_list = list( make_option(c(\"--argument_name\"), type=\"character\", default=NULL, help=\"Description of the argument\", metavar=\"character\") ); opt_parser = OptionParser(option_list=option_list); opt = parse_args(opt_parser); # An useful error if the argument is missing if (is.null(opt$argument_name)){ print_help(opt_parser) stop(\"Argument_name needs to be specified, but is missing.n\", call.=FALSE) } # Call the argument arg1 <- opt$argument_name","title":"4. The module script"},{"location":"start/modules/03_metric_module/#5-the-generate_metric_infopy-file","text":"This file could also be written in R or any other language. It should return a json file with the below fields with the metrics information. import argparse import json parser=argparse.ArgumentParser() parser.add_argument('--metric_info', help='Path to the metric info json file') args=parser.parse_args() metric_info = { 'flip': False, 'max': 1, 'min': 0, 'group': \"METRIC_GROUP\", 'name': \"metric_name\", 'input': \"metric_input_type\" } with open(args.metric_info, \"w\") as fp: json.dump(metric_info , fp, indent=3)","title":"5. The generate_metric_info.py file"},{"location":"start/modules/04_Add_module_to_omnibench/","text":"Add a module to an Omnibenchmark An omnibenchmark module will be able to import datasets from other modules and export its output to others. However, it still needs to be integrated in an Omnibenchmark orchestrator . An orchestrator is an omnibenchmark project which orchestrates the deployment, running and testing of each pieces of an Omnibenchmark. Integrate a module to an orchestrator The list of current Omnibenchmarks and their related orchestrator can be found on the Omnibenchmark dashboard : Omni-batch orchestrator Omni-clustering orchestrator To integrate your (populated and tested) module: Follow the link to the relevant orchestrator Open a new issue and describe briefly the aim of your module (data/ method/ metric module ?) The development team will check your module and integrate it in the Orchestrator worfklow. When it is done, you will be able to view the result of your module on the shiny app.","title":"Add a module to an Omnibenchmark"},{"location":"start/modules/04_Add_module_to_omnibench/#add-a-module-to-an-omnibenchmark","text":"An omnibenchmark module will be able to import datasets from other modules and export its output to others. However, it still needs to be integrated in an Omnibenchmark orchestrator . An orchestrator is an omnibenchmark project which orchestrates the deployment, running and testing of each pieces of an Omnibenchmark.","title":"Add a module to an Omnibenchmark"},{"location":"start/modules/04_Add_module_to_omnibench/#integrate-a-module-to-an-orchestrator","text":"The list of current Omnibenchmarks and their related orchestrator can be found on the Omnibenchmark dashboard : Omni-batch orchestrator Omni-clustering orchestrator To integrate your (populated and tested) module: Follow the link to the relevant orchestrator Open a new issue and describe briefly the aim of your module (data/ method/ metric module ?) The development team will check your module and integrate it in the Orchestrator worfklow. When it is done, you will be able to view the result of your module on the shiny app.","title":"Integrate a module to an orchestrator"}]}